{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9283a2",
   "metadata": {},
   "source": [
    "Piiranha =\n",
    "├── Base model (language understanding)\n",
    "│   └── produces vectors for each token\n",
    "├── Token-classification head\n",
    "│   └── converts vectors → label scores\n",
    "├── Trained weights\n",
    "│   └── learned during PII training\n",
    "└── Config\n",
    "    └── defines labels (PERSON, EMAIL, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from Hugging Face Hub: iiiorg/piiranha-v1-detect-personal-information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input IDs: tensor([[     1,   4017,    522,  17687,   1038,  11348,   4404,    260, 232825,\n",
      "         231638,    661,    292,      2]])\n",
      "Tokenized tokens: ['[CLS]', '▁Can', '▁you', '▁confirm', '▁my', '▁phone', '▁number', '▁', '9876', '5432', '10', '?', '[SEP]']\n",
      "Model outputs: TokenClassifierOutput(loss=None, logits=tensor([[[ 7.1680e-01, -9.5567e-01, -2.3313e+00, -2.5904e-01, -6.7706e-01,\n",
      "          -8.8575e-01, -1.0911e+00,  5.2335e-01, -7.2196e-01, -2.1118e+00,\n",
      "           1.3255e-02, -2.2428e+00, -1.3125e-01, -2.2871e-01,  2.9346e-01,\n",
      "          -1.2672e+00, -1.6063e+00,  4.9327e+00],\n",
      "         [-3.8637e-01, -2.6075e+00, -1.7384e+00, -1.4457e+00, -1.1260e+00,\n",
      "          -2.0737e+00, -2.8399e+00, -1.8835e+00, -1.5363e+00, -2.6820e+00,\n",
      "          -2.0250e+00, -1.8139e+00, -1.4232e+00, -2.2700e+00, -2.0360e+00,\n",
      "          -2.3014e+00, -3.2664e+00,  1.3467e+01],\n",
      "         [-6.0329e-01, -2.0453e+00, -2.0059e+00, -1.5335e+00, -1.0740e+00,\n",
      "          -1.9713e+00, -2.5437e+00, -2.2416e+00, -1.4520e+00, -3.1989e+00,\n",
      "          -2.0764e+00, -2.1109e+00, -1.5343e+00, -2.1532e+00, -2.2038e+00,\n",
      "          -2.1007e+00, -3.1823e+00,  1.3684e+01],\n",
      "         [-5.2968e-01, -1.7909e+00, -1.8672e+00, -1.4110e+00, -9.8950e-01,\n",
      "          -2.2608e+00, -2.4696e+00, -1.7684e+00, -1.6191e+00, -2.8799e+00,\n",
      "          -2.4178e+00, -1.7457e+00, -1.3989e+00, -2.3873e+00, -2.4145e+00,\n",
      "          -1.9383e+00, -3.0319e+00,  1.3534e+01],\n",
      "         [-6.5928e-01, -1.4932e+00, -2.3445e+00, -1.6514e+00, -7.8471e-01,\n",
      "          -2.1912e+00, -2.2560e+00, -1.8077e+00, -1.7695e+00, -2.3906e+00,\n",
      "          -2.4117e+00, -1.8746e+00, -1.6196e+00, -2.6190e+00, -2.2158e+00,\n",
      "          -1.6504e+00, -3.0392e+00,  1.3384e+01],\n",
      "         [-5.8602e-01, -1.3041e+00, -2.2177e+00, -1.7475e+00, -8.1919e-01,\n",
      "          -2.2490e+00, -2.1450e+00, -2.1315e+00, -1.7489e+00, -1.9669e+00,\n",
      "          -2.5757e+00, -1.6443e+00, -1.6383e+00, -2.5957e+00, -2.1507e+00,\n",
      "          -1.8483e+00, -2.8449e+00,  1.3303e+01],\n",
      "         [ 4.3758e-01, -1.6332e+00, -2.6500e+00, -1.6141e+00, -7.1225e-01,\n",
      "          -1.5909e+00, -2.4066e+00, -2.2559e+00, -1.6262e+00, -1.9358e+00,\n",
      "          -2.0221e+00, -1.9881e+00, -2.0749e+00, -2.1339e+00, -1.7535e+00,\n",
      "          -2.7567e+00, -3.1156e+00,  1.2875e+01],\n",
      "         [ 5.2242e+00, -1.7688e+00, -3.0574e+00, -1.6079e+00, -1.9446e+00,\n",
      "           6.7404e-01, -2.6504e+00, -2.6849e+00, -3.6153e-01, -2.5040e+00,\n",
      "          -4.0458e-01, -3.2659e+00, -2.7237e+00, -1.0353e+00, -4.4339e-01,\n",
      "          -2.8426e+00, -1.8939e+00,  8.0513e+00],\n",
      "         [ 5.3259e+00, -1.8236e+00, -2.7612e+00, -1.4067e+00, -1.9340e+00,\n",
      "           4.0073e-01, -2.8414e+00, -2.3860e+00, -6.2709e-01, -2.8197e+00,\n",
      "          -5.8667e-01, -3.2229e+00, -2.4847e+00, -1.0809e+00, -3.8491e-01,\n",
      "          -3.0440e+00, -1.6291e+00,  7.9315e+00],\n",
      "         [ 5.3514e+00, -1.8050e+00, -2.5027e+00, -1.6883e+00, -1.9254e+00,\n",
      "           5.7597e-01, -2.7670e+00, -2.4206e+00, -5.3986e-01, -2.9936e+00,\n",
      "          -4.9680e-01, -3.3311e+00, -2.3698e+00, -1.0105e+00, -3.2015e-01,\n",
      "          -2.9564e+00, -1.8059e+00,  7.7885e+00],\n",
      "         [ 5.3609e+00, -1.7881e+00, -2.5640e+00, -1.6797e+00, -1.9315e+00,\n",
      "           7.8047e-01, -2.8763e+00, -2.4639e+00, -4.0329e-01, -3.1152e+00,\n",
      "          -3.0368e-01, -3.3413e+00, -2.5065e+00, -9.6324e-01, -4.5388e-01,\n",
      "          -3.0777e+00, -1.7517e+00,  7.8800e+00],\n",
      "         [ 1.6929e+00, -1.4745e+00, -2.7081e+00, -1.9550e+00, -4.8266e-01,\n",
      "          -4.2408e-01, -3.1380e+00, -2.1093e+00, -2.1390e+00, -1.7431e+00,\n",
      "          -1.2684e+00, -2.8613e+00, -2.1046e+00, -1.6469e+00,  7.1624e-01,\n",
      "          -2.8226e+00, -2.6428e+00,  1.0373e+01],\n",
      "         [ 3.4639e+00, -1.8495e+00, -3.0934e+00, -1.3354e+00, -1.4207e+00,\n",
      "          -4.6997e-03, -2.3638e+00, -1.1246e+00, -7.5544e-01, -2.3781e+00,\n",
      "          -3.4221e-01, -2.9450e+00, -1.6652e+00, -4.2402e-01,  5.6791e-01,\n",
      "          -2.6533e+00, -1.1938e+00,  7.0501e+00]]]), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[[ 7.1680e-01, -9.5567e-01, -2.3313e+00, -2.5904e-01, -6.7706e-01,\n",
      "          -8.8575e-01, -1.0911e+00,  5.2335e-01, -7.2196e-01, -2.1118e+00,\n",
      "           1.3255e-02, -2.2428e+00, -1.3125e-01, -2.2871e-01,  2.9346e-01,\n",
      "          -1.2672e+00, -1.6063e+00,  4.9327e+00],\n",
      "         [-3.8637e-01, -2.6075e+00, -1.7384e+00, -1.4457e+00, -1.1260e+00,\n",
      "          -2.0737e+00, -2.8399e+00, -1.8835e+00, -1.5363e+00, -2.6820e+00,\n",
      "          -2.0250e+00, -1.8139e+00, -1.4232e+00, -2.2700e+00, -2.0360e+00,\n",
      "          -2.3014e+00, -3.2664e+00,  1.3467e+01],\n",
      "         [-6.0329e-01, -2.0453e+00, -2.0059e+00, -1.5335e+00, -1.0740e+00,\n",
      "          -1.9713e+00, -2.5437e+00, -2.2416e+00, -1.4520e+00, -3.1989e+00,\n",
      "          -2.0764e+00, -2.1109e+00, -1.5343e+00, -2.1532e+00, -2.2038e+00,\n",
      "          -2.1007e+00, -3.1823e+00,  1.3684e+01],\n",
      "         [-5.2968e-01, -1.7909e+00, -1.8672e+00, -1.4110e+00, -9.8950e-01,\n",
      "          -2.2608e+00, -2.4696e+00, -1.7684e+00, -1.6191e+00, -2.8799e+00,\n",
      "          -2.4178e+00, -1.7457e+00, -1.3989e+00, -2.3873e+00, -2.4145e+00,\n",
      "          -1.9383e+00, -3.0319e+00,  1.3534e+01],\n",
      "         [-6.5928e-01, -1.4932e+00, -2.3445e+00, -1.6514e+00, -7.8471e-01,\n",
      "          -2.1912e+00, -2.2560e+00, -1.8077e+00, -1.7695e+00, -2.3906e+00,\n",
      "          -2.4117e+00, -1.8746e+00, -1.6196e+00, -2.6190e+00, -2.2158e+00,\n",
      "          -1.6504e+00, -3.0392e+00,  1.3384e+01],\n",
      "         [-5.8602e-01, -1.3041e+00, -2.2177e+00, -1.7475e+00, -8.1919e-01,\n",
      "          -2.2490e+00, -2.1450e+00, -2.1315e+00, -1.7489e+00, -1.9669e+00,\n",
      "          -2.5757e+00, -1.6443e+00, -1.6383e+00, -2.5957e+00, -2.1507e+00,\n",
      "          -1.8483e+00, -2.8449e+00,  1.3303e+01],\n",
      "         [ 4.3758e-01, -1.6332e+00, -2.6500e+00, -1.6141e+00, -7.1225e-01,\n",
      "          -1.5909e+00, -2.4066e+00, -2.2559e+00, -1.6262e+00, -1.9358e+00,\n",
      "          -2.0221e+00, -1.9881e+00, -2.0749e+00, -2.1339e+00, -1.7535e+00,\n",
      "          -2.7567e+00, -3.1156e+00,  1.2875e+01],\n",
      "         [ 5.2242e+00, -1.7688e+00, -3.0574e+00, -1.6079e+00, -1.9446e+00,\n",
      "           6.7404e-01, -2.6504e+00, -2.6849e+00, -3.6153e-01, -2.5040e+00,\n",
      "          -4.0458e-01, -3.2659e+00, -2.7237e+00, -1.0353e+00, -4.4339e-01,\n",
      "          -2.8426e+00, -1.8939e+00,  8.0513e+00],\n",
      "         [ 5.3259e+00, -1.8236e+00, -2.7612e+00, -1.4067e+00, -1.9340e+00,\n",
      "           4.0073e-01, -2.8414e+00, -2.3860e+00, -6.2709e-01, -2.8197e+00,\n",
      "          -5.8667e-01, -3.2229e+00, -2.4847e+00, -1.0809e+00, -3.8491e-01,\n",
      "          -3.0440e+00, -1.6291e+00,  7.9315e+00],\n",
      "         [ 5.3514e+00, -1.8050e+00, -2.5027e+00, -1.6883e+00, -1.9254e+00,\n",
      "           5.7597e-01, -2.7670e+00, -2.4206e+00, -5.3986e-01, -2.9936e+00,\n",
      "          -4.9680e-01, -3.3311e+00, -2.3698e+00, -1.0105e+00, -3.2015e-01,\n",
      "          -2.9564e+00, -1.8059e+00,  7.7885e+00],\n",
      "         [ 5.3609e+00, -1.7881e+00, -2.5640e+00, -1.6797e+00, -1.9315e+00,\n",
      "           7.8047e-01, -2.8763e+00, -2.4639e+00, -4.0329e-01, -3.1152e+00,\n",
      "          -3.0368e-01, -3.3413e+00, -2.5065e+00, -9.6324e-01, -4.5388e-01,\n",
      "          -3.0777e+00, -1.7517e+00,  7.8800e+00],\n",
      "         [ 1.6929e+00, -1.4745e+00, -2.7081e+00, -1.9550e+00, -4.8266e-01,\n",
      "          -4.2408e-01, -3.1380e+00, -2.1093e+00, -2.1390e+00, -1.7431e+00,\n",
      "          -1.2684e+00, -2.8613e+00, -2.1046e+00, -1.6469e+00,  7.1624e-01,\n",
      "          -2.8226e+00, -2.6428e+00,  1.0373e+01],\n",
      "         [ 3.4639e+00, -1.8495e+00, -3.0934e+00, -1.3354e+00, -1.4207e+00,\n",
      "          -4.6997e-03, -2.3638e+00, -1.1246e+00, -7.5544e-01, -2.3781e+00,\n",
      "          -3.4221e-01, -2.9450e+00, -1.6652e+00, -4.2402e-01,  5.6791e-01,\n",
      "          -2.6533e+00, -1.1938e+00,  7.0501e+00]]])\n",
      "Logits shape: torch.Size([1, 13, 18])\n",
      "Predicted label IDs: tensor([[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]])\n",
      "Predicted labels for each token: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[CLS]           -> O\n",
      "▁Can            -> O\n",
      "▁you            -> O\n",
      "▁confirm        -> O\n",
      "▁my             -> O\n",
      "▁phone          -> O\n",
      "▁number         -> O\n",
      "▁               -> O\n",
      "9876            -> O\n",
      "5432            -> O\n",
      "10              -> O\n",
      "?               -> O\n",
      "[SEP]           -> O\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "# We import torch because the model runs on PyTorch, and PyTorch is the engine that actually does the math.\n",
    "import torch\n",
    "\n",
    "# transformers is a Python library made by Hugging Face.It contains Pretrained language models (BERT, RoBERTa, etc.), Tokenizers and utilities to load model from the Hugging Face Hub.\n",
    "# A tokenizer that automatically knows how to tokenize text for a given model. you do not need to say it which model you are using, just give it the model name and it will do the rest looking up the model config from the Hub.\n",
    "# AutoModelForTokenClassification Loads a pretrained model for token-level tasks (like PII or NER) with the token-classification head already attached. It reads the model’s config.json to know the task, labels, and output shape, and loads all the trained weights (both the base model and the head) from the model folder, so it’s ready to predict labels for each token without any extra setup.\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "MODEL_ID = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "LOCAL_MODEL_DIR = \"models_store/piiranha\"\n",
    "\n",
    "if os.path.isdir(LOCAL_MODEL_DIR) and os.listdir(LOCAL_MODEL_DIR):\n",
    "    print(f\"Loading model from local directory: {LOCAL_MODEL_DIR}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(LOCAL_MODEL_DIR)\n",
    "\n",
    "else:\n",
    "    print(f\"Downloading model from Hugging Face Hub: {MODEL_ID}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "    tokenizer.save_pretrained(LOCAL_MODEL_DIR)\n",
    "    model.save_pretrained(LOCAL_MODEL_DIR)\n",
    "\n",
    "text = \"Can you confirm my phone number 9876543210?\"\n",
    "\n",
    "# Tokenize text\n",
    "# return_tensors=\"pt\". This model is a PyTorch model, so we want the tokenizer to return PyTorch tensors, not plain python lists.\n",
    "# truncation=True. This ensures that if the input text is longer than the model's maximum input length, it will be truncated to fit.\n",
    "# The context length is 256 Deberta tokens(sub words), so approximately 160 - 200 words.\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, is_split_into_words=False)\n",
    "print(\"Tokenized input IDs:\", inputs[\"input_ids\"])\n",
    "print(\"Tokenized tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "\n",
    "# Forward pass (get logits)\n",
    "# Normally, PyTorch keeps track of all calculations so it can compute gradients for training. Since we are only doing inference here, we disable gradient tracking with torch.no_grad() to save memory and computation.\n",
    "# inputs is the tokenized sentence, which includes things like input IDs and attention masks.\n",
    "# Attention mask tells the model which tokens are real text (1) and which are padding (0).\n",
    "# If a sequence is shorter than max_length, the tokenizer pads it with zeros and marks those\n",
    "# padded positions with 0 in the attention mask so the model ignores them during attention.\n",
    "# The model processes the input and returns outputs, which include logits (raw vectors before applying argmax).\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "#logits are raw predictions from the model, before converting them to readable labels.\n",
    "#logits is a matrix of numbers, one row per token, one column per label.\n",
    "logits = outputs.logits\n",
    "print(\"Model outputs:\", outputs)\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Logits shape:\", logits.shape)  # (batch_size, num_tokens, num_labels)\n",
    "\n",
    "# Convert logits to predicted label IDs\n",
    "#argmax finds the index of the largest number along a specified dimension.\n",
    "#dim=-1 → look across the last dimension, which is labels for each token.\n",
    "#Tensor shape: (batch_size, num_tokens, num_labels)\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "print(\"Predicted label IDs:\", pred_ids)\n",
    "\n",
    "# Map label IDs to actual label names\n",
    "id2label = model.config.id2label\n",
    "pred_labels = [id2label[i.item()] for i in pred_ids[0]]\n",
    "print(\"Predicted labels for each token:\", pred_labels)\n",
    "\n",
    "# Map tokens to words (manual aggregation)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "for token, label in zip(tokens, pred_labels):\n",
    "    print(f\"{token:15} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a1beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory: models_store/roblox\n",
      "Tokenized input IDs: tensor([[   0,  831,  398, 8337,  935, 3340,  705,    2]])\n",
      "Tokenized tokens: ['<s>', '▁can', '▁you', '▁give', '▁your', '▁email', '▁?', '</s>']\n",
      "Model outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7898, -3.7482]]), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[ 2.7898, -3.7482]])\n",
      "Logits shape: torch.Size([1, 2])\n",
      "Predicted probabilities: tensor([0.9986, 0.0014])\n",
      "Label probabilities: {'p_privacy_asking_for_pii': 0.9985546469688416, 'p_privacy_giving_pii': 0.0014453405747190118}\n",
      "\n",
      "Text: what is your phone number?\n",
      "Max probability: 0.9986 → Combined cutoff (0.2691) → PII detected? True\n",
      "Asking PII cutoff (0.2) → Detected? True\n",
      "Giving PII cutoff (0.3) → Detected? False\n",
      "Final decision (any threshold passed) → PII detected? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_ID = \"Roblox/roblox-pii-classifier\"\n",
    "LOCAL_MODEL_DIR = \"models_store/roblox\"\n",
    "\n",
    "if os.path.isdir(LOCAL_MODEL_DIR) and os.listdir(LOCAL_MODEL_DIR):\n",
    "    print(f\"Loading model from local directory: {LOCAL_MODEL_DIR}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR, fix_mistral_regex=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)\n",
    "\n",
    "else:\n",
    "    print(f\"Downloading model from Hugging Face Hub: {MODEL_ID}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, fix_mistral_regex=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "    tokenizer.save_pretrained(LOCAL_MODEL_DIR)\n",
    "    model.save_pretrained(LOCAL_MODEL_DIR)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"can you give your email?\", return_tensors=\"pt\", truncation=True, is_split_into_words=False\n",
    ")\n",
    "print(\"Tokenized input IDs:\", inputs[\"input_ids\"])\n",
    "print(\"Tokenized tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(\"Model outputs:\", outputs)\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Logits shape:\", logits.shape)  \n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probs = torch.softmax(logits, dim=-1)[0]\n",
    "print(\"Predicted probabilities:\", probs)\n",
    "\n",
    "# Map label IDs to actual label names\n",
    "id2label = model.config.id2label\n",
    "results = {id2label[i]: float(probs[i]) for i in range(len(probs))}\n",
    "print(\"Label probabilities:\", results)\n",
    "\n",
    "# Cutoffs\n",
    "combined_cutoff = 0.2691\n",
    "asking_cutoff = 0.2\n",
    "giving_cutoff = 0.3\n",
    "\n",
    "# Combined cutoff check\n",
    "max_prob = max(results.values())\n",
    "is_pii_combined = max_prob >= combined_cutoff\n",
    "\n",
    "# Individual cutoffs\n",
    "is_pii_asking = results['p_privacy_asking_for_pii'] >= asking_cutoff\n",
    "is_pii_giving = results['p_privacy_giving_pii'] >= giving_cutoff\n",
    "\n",
    "# Final PII decision\n",
    "is_pii_any = is_pii_asking or is_pii_giving\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nText: {'what is your phone number?'}\")\n",
    "print(f\"Max probability: {max_prob:.4f} → Combined cutoff ({combined_cutoff}) → PII detected? {is_pii_combined}\")\n",
    "print(f\"Asking PII cutoff ({asking_cutoff}) → Detected? {is_pii_asking}\")\n",
    "print(f\"Giving PII cutoff ({giving_cutoff}) → Detected? {is_pii_giving}\")\n",
    "print(f\"Final decision (any threshold passed) → PII detected? {is_pii_any}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PII Clasiffier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
